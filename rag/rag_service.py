#!/usr/bin/env python3
import os
from typing import List, Tuple

import faiss
import requests
from fastapi import FastAPI
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from pydantic import BaseModel
from sentence_transformers import SentenceTransformer


FUSEKI_URL = os.environ.get("FUSEKI_URL", "http://localhost:3030/vn")
QUERY_URL = f"{FUSEKI_URL}/query"

# LLM API Configuration
GEMINI_API_KEY = os.environ.get("GEMINI_API_KEY")


class QueryRequest(BaseModel):
    question: str


class LLMResponse(BaseModel):
    answer: str
    context_used: List[str]
    confidence: float


def query_triples_for_context() -> List[str]:
    sparql = """
    PREFIX ex: <http://example.org/vn/ontology#>

    SELECT ?new_province ?new_label ?old_province ?old_label WHERE {
    ?new_province ex:formedBy ?old_province .
    ?new_province ex:canonicalLabel ?new_label .
    ?old_province ex:canonicalLabel ?old_label .
    }
    ORDER BY ?new_label ?old_label
    """
    r = requests.get(
        QUERY_URL,
        params={"query": sparql, "format": "application/sparql-results+json"},
        timeout=30,
    )
    r.raise_for_status()
    data = r.json()["results"]["bindings"]
    docs: List[str] = []
    for b in data:
        docs.append(str(b))
    return docs


class RAGIndex:
    def __init__(self) -> None:
        self.model = SentenceTransformer("all-MiniLM-L6-v2")
        self.index = None
        self.corpus: List[str] = []

    def build(self) -> None:
        self.corpus = query_triples_for_context()
        if not self.corpus:
            self.index = None
            return
        emb = self.model.encode(
            self.corpus, convert_to_numpy=True, show_progress_bar=False
        )
        d = emb.shape[1]
        self.index = faiss.IndexFlatIP(d)
        faiss.normalize_L2(emb)
        self.index.add(emb)

    def search(self, query: str, k: int = 5) -> List[Tuple[str, float]]:
        if not self.index:
            return []
        q = self.model.encode([query], convert_to_numpy=True)
        faiss.normalize_L2(q)
        scores, idx = self.index.search(q, k)
        results: List[Tuple[str, float]] = []
        for i, s in zip(idx[0], scores[0]):
            if i >= 0 and i < len(self.corpus):
                results.append((self.corpus[i], float(s)))
        return results


class LLMService:
    """Service ƒë·ªÉ g·ªçi Gemini API"""

    def __init__(self):
        self.gemini_api_key = GEMINI_API_KEY

    def generate_answer(self, question: str, context: List[Tuple[str, float]]) -> str:
        """T·∫°o c√¢u tr·∫£ l·ªùi t·ª´ context s·ª≠ d·ª•ng Gemini API"""

        if not context:
            return "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin li√™n quan ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa b·∫°n."

        # Chu·∫©n b·ªã context
        context_text = self._prepare_context(context)

        return self._call_gemini(question, context_text)

    def _prepare_context(self, context: List[Tuple[str, float]]) -> str:
        return context

    def _call_gemini(self, question: str, context: str) -> str:
        """G·ªçi Google Gemini API"""
        if not self.gemini_api_key:
            return self._fallback_response(question, context)

        try:
            url = (
                "https://generativelanguage.googleapis.com/v1beta/"
                "models/gemini-2.0-flash:generateContent"
            )

            prompt = f"""D·ª±a tr√™n d·ªØ li·ªáu SPARQL sau v·ªÅ c√°c t·ªânh th√†nh Vi·ªát Nam, h√£y tr·∫£ l·ªùi c√¢u h·ªèi b·∫±ng ti·∫øng Vi·ªát:

D·ªØ li·ªáu (format: new_province: URI t·ªânh sau s√°t nh·∫≠p, new_label: t√™n t·ªânh sau s√°t nh·∫≠p, old_province: URI t·ªânh tr∆∞·ªõc s√°t nh·∫≠p, old_label: t√™n t·ªânh tr∆∞·ªõc s√°t nh·∫≠p):
{context}

C√¢u h·ªèi: {question}

H√£y ph√¢n t√≠ch d·ªØ li·ªáu v√† tr·∫£ l·ªùi ch√≠nh x√°c, ƒë∆∞a ra c√°c uri li√™n quan n·∫øu c√≥:"""

            print(prompt)

            payload = {
                "contents": [
                    {
                        "parts": [
                            {
                                "text": prompt
                            }
                        ]
                    }
                ]
            }

            headers = {
                "Content-Type": "application/json",
                "X-goog-api-key": self.gemini_api_key
            }

            response = requests.post(url, json=payload, headers=headers, timeout=30)
            result = response.json()

            if response.status_code == 200:
                if "candidates" in result and len(result["candidates"]) > 0:
                    candidate = result["candidates"][0]
                    if "content" in candidate and "parts" in candidate["content"]:
                        text = candidate["content"]["parts"][0]["text"]
                        print(f"Extracted text: {text}")
                        return text
                    else:
                        print(f"Candidate structure: {candidate}")
                else:
                    print("No candidates found in response")

            return self._fallback_response(question, context)

        except Exception as e:
            print(f"L·ªói Gemini API: {e}")
            return self._fallback_response(question, context)

    def _fallback_response(self, question: str, context: str) -> str:
        print("Kh√¥ng t√¨m ƒë∆∞·ª£c")
        """C√¢u tr·∫£ l·ªùi d·ª± ph√≤ng khi kh√¥ng th·ªÉ g·ªçi Gemini API"""
        return f"""D·ª±a tr√™n th√¥ng tin t√¨m ƒë∆∞·ª£c, ƒë√¢y l√† nh·ªØng g√¨ li√™n quan ƒë·∫øn c√¢u h·ªèi "{question}":

{context}

ƒê√¢y l√† th√¥ng tin th√¥ t·ª´ c∆° s·ªü d·ªØ li·ªáu. B·∫°n c√≥ th·ªÉ c·∫ßn ph√¢n t√≠ch th√™m ƒë·ªÉ hi·ªÉu r√µ h∆°n."""


app = FastAPI(title="RAG Chatbot API", description="API cho h·ªá th·ªëng RAG chatbot")

# Mount static files
app.mount("/static", StaticFiles(directory="static"), name="static")

rag_index = RAGIndex()
llm_service = LLMService()


@app.on_event("startup")
def startup_event() -> None:
    rag_index.build()
    print("ü§ñ LLM Provider: Gemini")
    if not GEMINI_API_KEY:
        print("‚ö†Ô∏è  GEMINI_API_KEY ch∆∞a ƒë∆∞·ª£c c·∫•u h√¨nh")
    else:
        print("‚úÖ Gemini API key ƒë√£ ƒë∆∞·ª£c c·∫•u h√¨nh")


@app.post("/ask")
def ask(req: QueryRequest):
    # T√¨m ki·∫øm context
    # hits = rag_index.search(req.question, k=8)
    hits = query_triples_for_context()

    # T·∫°o c√¢u tr·∫£ l·ªùi t·ª´ LLM
    answer = llm_service.generate_answer(req.question, hits)

    return {
        "question": req.question,
        "answer": answer,
        "context": '',
        "confidence": 1,
        "llm_provider": "gemini",
    }


@app.get("/")
def read_root():
    return FileResponse("static/index.html")
